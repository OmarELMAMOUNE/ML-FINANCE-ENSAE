{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23897393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Load Federal Funds Effective Rate (actual historical rates)\n",
    "EFFR = pd.read_csv(\"FEDFUNDS.csv\", sep=\",\")\n",
    "\n",
    "\n",
    "# Load Federal Funds Futures (market expectations)\n",
    "FFF = pd.read_csv(\"Federal_Fund_Future.csv\", sep=\";\")\n",
    "\n",
    "\n",
    "# Convert date columns to datetime\n",
    "EFFR['observation_date'] = pd.to_datetime(EFFR['observation_date'])\n",
    "FFF['Date'] = pd.to_datetime(FFF['Date'])\n",
    "\n",
    "# Clean futures data - keep only necessary columns\n",
    "FFF = FFF.drop(columns=['Open', 'High', 'Low', 'Vol.', 'Change %'], errors='ignore')\n",
    "\n",
    "# Calculate implicit fund rate from futures price\n",
    "# Futures price = 100 - implied rate (standard convention)\n",
    "FFF['implicite_fund_rate'] = 100 - FFF['Price']\n",
    "\n",
    "# Merge datasets on date\n",
    "merged_df = pd.merge(\n",
    "    FFF[['Date', 'implicite_fund_rate']], \n",
    "    EFFR[['observation_date', 'FEDFUNDS']], \n",
    "    left_on='Date', \n",
    "    right_on='observation_date', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "\n",
    "# Load macro features\n",
    "macro_df = pd.read_csv(\"macro_features_monthly.csv\")\n",
    "macro_df['DATE'] = pd.to_datetime(macro_df['DATE'], dayfirst=True)\n",
    "\n",
    "\n",
    "# Convert to monthly period for proper alignment\n",
    "macro_df['DATE'] = macro_df['DATE'].values.astype('datetime64[M]')\n",
    "\n",
    "\n",
    "\n",
    "merged_all = pd.merge(\n",
    "    merged_df, \n",
    "    macro_df, \n",
    "    left_on='Date', \n",
    "    right_on='DATE', \n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb0a7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Sort by date and reset index\n",
    "merged_all = merged_all.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Create working copy\n",
    "df_features = merged_all.copy()\n",
    "\n",
    "# Calculate spread (market expectation vs actual rate)\n",
    "df_features['spread_change'] = df_features['implicite_fund_rate'] - df_features['FEDFUNDS']\n",
    "\n",
    "#Calculate CHANGES (first differences) for stationarity\n",
    "\n",
    "for el in [\"implicite_fund_rate\",'CPIAUCSL', 'PCEPI', 'UNRATE', 'PAYEMS', 'GDP', 'INDPRO','FinStress']:\n",
    "       df_features[f'{el}_change']=df_features[el].diff()\n",
    "\n",
    "\n",
    "n_lags = 6  # Use past 6 months\n",
    "\n",
    "# Lagged variables creation\n",
    "\n",
    "for el in [\"spread\",\"implicite_fund_rate\",'CPIAUCSL', 'PCEPI', 'UNRATE', 'PAYEMS', 'GDP', 'INDPRO','FinStress']:\n",
    "    for i in range(1, n_lags + 1):\n",
    "        df_features[f'{el}_change_lag_{i}'] = df_features[f'{el}_change'].shift(i)\n",
    "\n",
    "\n",
    "df_features['future_fedfunds_change'] = df_features['FEDFUNDS'].diff().shift(-1)\n",
    "\n",
    "# Binary classification: 1 if rate increases, 0 otherwise\n",
    "df_features['target'] = (df_features['future_fedfunds_change'] > 0).astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24afb706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced dataset for accuracy :  target\n",
      "0    146\n",
      "1    119\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_final=df_features.dropna()\n",
    "print(\"balanced dataset for accuracy : \",df_final['target'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68bc155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow\n",
    "import optuna\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "def check_params(allowed,kwargs):\n",
    "\n",
    "\n",
    "    for key,val in kwargs.items():\n",
    "\n",
    "                if key not in allowed:\n",
    "                    raise ValueError(f\"{key} unrecongized, use : {allowed.keys()}\")\n",
    "                if val not in allowed[key]:\n",
    "                    raise ValueError(f\"{val} unrecongized, for {key} use : {allowed[key]}\")\n",
    "\n",
    "class MlFlowModel:\n",
    "\n",
    "    def __init__(self,exper_name,model_instance,**kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        supports two models: logistic regression and random forest only \n",
    "\n",
    "        \"\"\"\n",
    "        self.model_instance=model_instance\n",
    "        self.exper_name=exper_name\n",
    "        self.kwargs=kwargs\n",
    "        self.n_trials=10\n",
    "\n",
    "        if isinstance(self.model_instance,type):\n",
    "            raise ValueError(f\"dont forget parantheses in your model_instance\")\n",
    "        \n",
    "\n",
    "        if self.model_instance.__class__ is not   LogisticRegression and  self.model_instance.__class__ is not RandomForestClassifier:\n",
    "            raise ValueError('sorry, we can use logistic regression and random forest only for the moment ')\n",
    "\n",
    "\n",
    "        if self.model_instance.__class__ is  LogisticRegression:\n",
    "            allowed={\n",
    "                \"penalty\" : [\"l1\", \"l2\", \"elasticnet\", None],\n",
    "                \"solver\" : \"saga\"\n",
    "            }\n",
    "\n",
    "            if \"penalty\" not in kwargs.keys() or \"solver\" not in kwargs.keys():\n",
    "                raise ValueError(\"need to specify both penalty and solver for logistic regression \")\n",
    "\n",
    "        if self.model_instance.__class__ is  RandomForestClassifier:\n",
    "            allowed={\n",
    "                \"max_features\" : [\"sqrt\", \"log2\", None],\n",
    "                \"criterion\" : [\"gini\", \"entropy\", \"log_loss\"]\n",
    "            }\n",
    "            if \"max_features\" not in kwargs.keys() or \"criterion\" not in kwargs.keys():\n",
    "                raise ValueError(\"need to specify both max_features and criterion for randm forest \")\n",
    "\n",
    "        \n",
    "        check_params(allowed,self.kwargs)\n",
    "\n",
    "    \n",
    "    def objective(self,trial):\n",
    "                \n",
    "\n",
    "                if self.model_instance.__class__ is  LogisticRegression:\n",
    "\n",
    "                    if \"elasticnet\" == self.kwargs[\"penalty\"]:\n",
    "                        # Define hyperparameter search space\n",
    "                        params_candidate_space={\n",
    "                            \"l1_ratio\": trial.suggest_float(\"l1_ratio\", 0.1, 0.9),\n",
    "                            \"C\":  trial.suggest_float(\"C\", 0.1, 10),\n",
    "                        }\n",
    "                    elif \"l1\" == self.kwargs[\"penalty\"] or \"l2\" == self.kwargs[\"penalty\"]:\n",
    "                        # Define hyperparameter search space\n",
    "                        params_candidate_space={\n",
    "                            \"C\":  trial.suggest_float(\"C\", 0.1, 10),\n",
    "                        }\n",
    "                    \n",
    "                if self.model_instance.__class__ is  RandomForestClassifier:\n",
    "\n",
    "                    params_candidate_space={\n",
    "                            'max_depth': trial.suggest_int(\"max_depth\", 10, 50),\n",
    "                            \"min_samples_split\":  trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "                            \"min_samples_leaf\":  trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "                 \n",
    "                        }\n",
    "\n",
    "                \n",
    "\n",
    "                model = self.model_instance.__class__(\n",
    "                    **params_candidate_space, \n",
    "                    **self.fixed_params,\n",
    "                    **self.kwargs  \n",
    "                )\n",
    "            \n",
    "\n",
    "                model.fit(self.X_train, self.y_train)\n",
    "\n",
    "\n",
    "                #log_loss is sklearn is negative loglikelihood so we minimise it\n",
    "                #in randomforest if we use gini, then we need to modify this line, but we do simple:\n",
    "                score = log_loss(self.y_train, model.predict_proba(self.X_train))\n",
    "\n",
    "                return score\n",
    "\n",
    "\n",
    "\n",
    "    def run_optuna_study(self):\n",
    "\n",
    "        storage = f\"sqlite:///optuna_{self.exper_name}.db\"\n",
    "        study = optuna.create_study(\n",
    "                            direction=\"minimize\", \n",
    "                            study_name=f\"{self.exper_name}\",\n",
    "                            storage=storage,\n",
    "                            load_if_exists=True\n",
    "        )  # 'minimize' for loss functions\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        study_best_params=study.best_params\n",
    "        return study_best_params\n",
    "\n",
    "    def train(self,merged_df_,feature_columns):\n",
    "        \n",
    "        with mlflow.start_run(run_name=f\"{self.exper_name}_FedFunds\"):\n",
    "        \n",
    "                mlflow.log_param(\"model_type\", self.model_instance.__class__.__name__)\n",
    "                mlflow.log_param(\"variables\", feature_columns)\n",
    "                mlflow.log_param(\"scaler\", \"StandardScaler\")\n",
    "                mlflow.log_param(\"train_test_split\", \"80/20 time series\")\n",
    "                mlflow.log_param(\"kwargs\",self.kwargs)\n",
    "                \n",
    "                #do easy splits and transforms:\n",
    "                X = merged_df_[feature_columns]\n",
    "                y = merged_df_['target']\n",
    "                split_idx = int(len(merged_df_) * 0.8)\n",
    "                self.X_train, self.X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "                self.y_train, self.y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "                #always standartise variables for logit \n",
    "                if self.model_instance.__class__ is  LogisticRegression:\n",
    "                    scaler = StandardScaler()\n",
    "                    self.X_train = scaler.fit_transform(self.X_train)\n",
    "                    self.X_test = scaler.transform(self.X_test)\n",
    "\n",
    "                # Model penalisations\n",
    "\n",
    "                if self.model_instance.__class__ is  LogisticRegression:\n",
    "                    print(\"normalising data\")\n",
    "                    self.fixed_params={\n",
    "                        \n",
    "                        \"class_weight\" : 'balanced',  \n",
    "                        \"random_state\" : 42,\n",
    "                    }\n",
    "                \n",
    "\n",
    "                    if self.kwargs[\"penalty\"] is not None:\n",
    "                        #in this case need to search space for best hyperparameter\n",
    "                        study_best_params=self.run_optuna_study()\n",
    "                        print(\"Best Hyperparameters:\",study_best_params)\n",
    "                    else:\n",
    "                        study_best_params={}\n",
    "\n",
    "                if self.model_instance.__class__ is  RandomForestClassifier:\n",
    "                    self.fixed_params={\n",
    "                        \"n_estimators\" : 150,\n",
    "                        \"class_weight\" : 'balanced',  \n",
    "                       \n",
    "                    }\n",
    "                    study_best_params=self.run_optuna_study()\n",
    "                    print(\"Best Hyperparameters:\",study_best_params)\n",
    "                    \n",
    "\n",
    "                        \n",
    "                model = self.model_instance.__class__(\n",
    "                     **self.fixed_params,\n",
    "                     **study_best_params,\n",
    "                     **self.kwargs                \n",
    "                )\n",
    "                print(\"-----------\")\n",
    "                \n",
    "\n",
    "               \n",
    "                model.fit(self.X_train, self.y_train)\n",
    "                # Predictions\n",
    "                y_pred = model.predict(self.X_test)\n",
    "                \n",
    "                acc = accuracy_score(self.y_test, y_pred)\n",
    "                mlflow.log_metric(\"accuracy\", acc)\n",
    "                mlflow.sklearn.log_model(model, name=\"model\")\n",
    "\n",
    "                print(f\"Run logged to MLflow: accuracy={acc:.4f}\")\n",
    "\n",
    "\n",
    "def var_selection_with_permutation(model,X,y,threshold_below_which_to_drop=0.01):\n",
    "\n",
    "    '''\n",
    "    supports any classification model, models implying gradient descent (parametric models) require dataset to be normalised \n",
    "    models such as random forests or other non gradient tree models do not require variable standartisation \n",
    "    '''\n",
    "    \n",
    "    split_idx = int(len(X) * 0.8)\n",
    "\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "    if model.__class__.__name__ in [\"LogisticRegression\",]:\n",
    "        \n",
    "        scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42,scoring=\"accuracy\")\n",
    "\n",
    "    perm_importances = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'importance_mean': result.importances_mean,\n",
    "        'importance_std': result.importances_std\n",
    "    })\n",
    "    \n",
    "    return perm_importances[perm_importances[\"importance_mean\"]>threshold_below_which_to_drop][\"feature\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "852ad694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb variables before permutation selection ': 3, 'nb variables after permutation selection ': 3}\n",
      "normalising data\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "\u001b[31m2025/11/13 22:33:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run logged to MLflow: accuracy=0.6038\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "feature_columns =  [f'spread_change_lag_{i}' for i in range(1, 3 + 1)] \n",
    "X = final_df[feature_columns]\n",
    "y = final_df['target']\n",
    "feature_columns_after_permutation_test=var_selection_with_permutation(model,X,y)\n",
    "print(\n",
    "{\n",
    "    \"nb variables before permutation selection \" : len(feature_columns),\n",
    "    \"nb variables after permutation selection \" : len(feature_columns_after_permutation_test)\n",
    "\n",
    "}\n",
    ")\n",
    "model=MlFlowModel(\"lrsimplest\",LogisticRegression(),penalty=None,solver=\"saga\")\n",
    "model.train(final_df,feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95205cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalising data\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/11/13 22:54:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run logged to MLflow: accuracy=0.6226\n"
     ]
    }
   ],
   "source": [
    "feature_columns=[col for col in df_final.columns if 'spread' in col.lower()  and col[-1].isdigit() ]\n",
    "\n",
    "\n",
    "model=MlFlowModel(\"lrsimplest\",LogisticRegression(),penalty=None,solver=\"saga\")\n",
    "model.train(df_final,feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9f0fdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalising data\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/11/13 22:55:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run logged to MLflow: accuracy=0.6604\n"
     ]
    }
   ],
   "source": [
    "feature_columns=[col for col in df_final.columns if 'change' in col.lower()  and col[-1].isdigit() ]\n",
    "\n",
    "\n",
    "model=MlFlowModel(\"lr-all-vars\",LogisticRegression(),penalty=None,solver=\"saga\")\n",
    "model.train(df_final,feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "077e15f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalising data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 22:55:27,376] A new study created in RDB with name: lr-all-vars\n",
      "[I 2025-11-13 22:55:27,541] Trial 0 finished with value: 0.5051579097197709 and parameters: {'l1_ratio': 0.6085026294298703, 'C': 6.05591792396805}. Best is trial 0 with value: 0.5051579097197709.\n",
      "[I 2025-11-13 22:55:27,682] Trial 1 finished with value: 0.5051176951185505 and parameters: {'l1_ratio': 0.3140243690147676, 'C': 4.397897380859839}. Best is trial 1 with value: 0.5051176951185505.\n",
      "[I 2025-11-13 22:55:27,831] Trial 2 finished with value: 0.5105740347495147 and parameters: {'l1_ratio': 0.8079833866071162, 'C': 1.939630114780993}. Best is trial 1 with value: 0.5051176951185505.\n",
      "[I 2025-11-13 22:55:27,997] Trial 3 finished with value: 0.5062370193997643 and parameters: {'l1_ratio': 0.5435535037559394, 'C': 3.575123780844866}. Best is trial 1 with value: 0.5051176951185505.\n",
      "[I 2025-11-13 22:55:28,170] Trial 4 finished with value: 0.5050139301039904 and parameters: {'l1_ratio': 0.16777888921494605, 'C': 3.7414640543944437}. Best is trial 4 with value: 0.5050139301039904.\n",
      "[I 2025-11-13 22:55:28,303] Trial 5 finished with value: 0.5053031136892447 and parameters: {'l1_ratio': 0.587070544395154, 'C': 5.488446278806879}. Best is trial 4 with value: 0.5050139301039904.\n",
      "[I 2025-11-13 22:55:28,470] Trial 6 finished with value: 0.510879276596116 and parameters: {'l1_ratio': 0.8824885470121119, 'C': 1.9893946935815474}. Best is trial 4 with value: 0.5050139301039904.\n",
      "[I 2025-11-13 22:55:28,647] Trial 7 finished with value: 0.5053188284948094 and parameters: {'l1_ratio': 0.8141837194261595, 'C': 6.7013589184370455}. Best is trial 4 with value: 0.5050139301039904.\n",
      "[I 2025-11-13 22:55:28,821] Trial 8 finished with value: 0.5058425635575539 and parameters: {'l1_ratio': 0.5488834066789811, 'C': 4.148012680282407}. Best is trial 4 with value: 0.5050139301039904.\n",
      "[I 2025-11-13 22:55:29,017] Trial 9 finished with value: 0.5052469162461556 and parameters: {'l1_ratio': 0.10632078656481987, 'C': 2.9299880363508124}. Best is trial 4 with value: 0.5050139301039904.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'l1_ratio': 0.16777888921494605, 'C': 3.7414640543944437}\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/11/13 22:55:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run logged to MLflow: accuracy=0.6604\n"
     ]
    }
   ],
   "source": [
    "feature_columns=[col for col in df_final.columns if 'change' in col.lower()  and col[-1].isdigit() ]\n",
    "\n",
    "\n",
    "model=MlFlowModel(\"lr-all-vars\",LogisticRegression(),penalty=\"elasticnet\",solver=\"saga\")\n",
    "model.train(df_final,feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 22:57:01,717] Using an existing study with name 'lr-per-vars0.01-elnet' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb variables before permutation selection ': 54, 'nb variables after permutation selection ': 13}\n",
      "normalising data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 22:57:02,384] Trial 10 finished with value: 0.5485562006802969 and parameters: {'l1_ratio': 0.5345677177674106, 'C': 9.692397403325243}. Best is trial 10 with value: 0.5485562006802969.\n",
      "[I 2025-11-13 22:57:02,535] Trial 11 finished with value: 0.5485509180132122 and parameters: {'l1_ratio': 0.5309322206305545, 'C': 9.936698549812881}. Best is trial 11 with value: 0.5485509180132122.\n",
      "[I 2025-11-13 22:57:02,660] Trial 12 finished with value: 0.5485518911853527 and parameters: {'l1_ratio': 0.5551783486995054, 'C': 9.968375676833631}. Best is trial 11 with value: 0.5485509180132122.\n",
      "[I 2025-11-13 22:57:02,785] Trial 13 finished with value: 0.5486016179394072 and parameters: {'l1_ratio': 0.43253833872312986, 'C': 7.711986725941861}. Best is trial 11 with value: 0.5485509180132122.\n",
      "[I 2025-11-13 22:57:02,904] Trial 14 finished with value: 0.5487350181468039 and parameters: {'l1_ratio': 0.5863154920008881, 'C': 5.576269618877306}. Best is trial 11 with value: 0.5485509180132122.\n",
      "[I 2025-11-13 22:57:03,039] Trial 15 finished with value: 0.5487006354145395 and parameters: {'l1_ratio': 0.8866199932284158, 'C': 6.6990331120587}. Best is trial 11 with value: 0.5485509180132122.\n",
      "[I 2025-11-13 22:57:03,173] Trial 16 finished with value: 0.5485735904514175 and parameters: {'l1_ratio': 0.3579315833102236, 'C': 8.41951953679456}. Best is trial 11 with value: 0.5485509180132122.\n",
      "[I 2025-11-13 22:57:03,315] Trial 17 finished with value: 0.5488599537434021 and parameters: {'l1_ratio': 0.616900009847724, 'C': 4.482826011623417}. Best is trial 11 with value: 0.5485509180132122.\n",
      "[I 2025-11-13 22:57:03,454] Trial 18 finished with value: 0.5485679852452575 and parameters: {'l1_ratio': 0.3935502246330108, 'C': 8.740955741217274}. Best is trial 11 with value: 0.5485509180132122.\n",
      "[I 2025-11-13 22:57:03,579] Trial 19 finished with value: 0.5486422104788621 and parameters: {'l1_ratio': 0.5330489060890828, 'C': 6.957726797460191}. Best is trial 11 with value: 0.5485509180132122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'l1_ratio': 0.5309322206305545, 'C': 9.936698549812881}\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/11/13 22:57:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run logged to MLflow: accuracy=0.7170\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "feature_columns=[col for col in df_final.columns if 'change' in col.lower()  and col[-1].isdigit() ]\n",
    "X = df_final[feature_columns]\n",
    "y = df_final['target']\n",
    "feature_columns_after_permutation_test=var_selection_with_permutation(model,X,y)\n",
    "print(\n",
    "{\n",
    "    \"nb variables before permutation selection \" : len(feature_columns),\n",
    "    \"nb variables after permutation selection \" : len(feature_columns_after_permutation_test)\n",
    "\n",
    "}\n",
    ")\n",
    "model=MlFlowModel(\"lr-per-vars0.01-elnet\",LogisticRegression(),penalty=\"elasticnet\",solver=\"saga\")\n",
    "model.train(df_final,feature_columns_after_permutation_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d2a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "feature_columns=[col for col in df_final.columns if 'change' in col.lower()  and col[-1].isdigit() ]\n",
    "X = df_final[feature_columns]\n",
    "y = df_final['target']\n",
    "feature_columns_after_permutation_test=var_selection_with_permutation(model,X,y)\n",
    "print(\n",
    "{\n",
    "    \"nb variables before permutation selection \" : len(feature_columns),\n",
    "    \"nb variables after permutation selection \" : len(feature_columns_after_permutation_test)\n",
    "\n",
    "}\n",
    ")\n",
    "model=MlFlowModel(\"lr-per-vars0.01-elnet\",LogisticRegression(),penalty=\"elasticnet\",solver=\"saga\")\n",
    "model.train(df_final,feature_columns_after_permutation_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a8d6a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 22:58:19,174] Using an existing study with name 'rf-per-vars0.02-sqrt-logloss' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nb variables before permutation selection ': 54, 'nb variables after permutation selection ': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-13 22:58:19,604] Trial 10 finished with value: 0.1682347534295147 and parameters: {'max_depth': 31, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.16018747069960343.\n",
      "[I 2025-11-13 22:58:20,019] Trial 11 finished with value: 0.16829147929633806 and parameters: {'max_depth': 31, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.16018747069960343.\n",
      "[I 2025-11-13 22:58:20,441] Trial 12 finished with value: 0.16920938820941034 and parameters: {'max_depth': 37, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.16018747069960343.\n",
      "[I 2025-11-13 22:58:20,842] Trial 13 finished with value: 0.18926948630812726 and parameters: {'max_depth': 28, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.16018747069960343.\n",
      "[I 2025-11-13 22:58:21,262] Trial 14 finished with value: 0.25716567185937006 and parameters: {'max_depth': 44, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.16018747069960343.\n",
      "[I 2025-11-13 22:58:21,674] Trial 15 finished with value: 0.1813766060825824 and parameters: {'max_depth': 30, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.16018747069960343.\n",
      "[I 2025-11-13 22:58:22,106] Trial 16 finished with value: 0.3075367040699506 and parameters: {'max_depth': 50, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 5 with value: 0.16018747069960343.\n",
      "[I 2025-11-13 22:58:22,565] Trial 17 finished with value: 0.26922278366099067 and parameters: {'max_depth': 23, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.16018747069960343.\n",
      "[I 2025-11-13 22:58:22,956] Trial 18 finished with value: 0.18792329965848845 and parameters: {'max_depth': 37, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 5 with value: 0.16018747069960343.\n",
      "[I 2025-11-13 22:58:23,351] Trial 19 finished with value: 0.24692277336830476 and parameters: {'max_depth': 26, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 5 with value: 0.16018747069960343.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 26, 'min_samples_split': 3, 'min_samples_leaf': 1}\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/11/13 22:58:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run logged to MLflow: accuracy=0.8302\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression( random_state=42,class_weight='balanced')\n",
    "feature_columns=[col for col in df_final.columns if 'change' in col.lower()  and col[-1].isdigit() ]\n",
    "X = df_final[feature_columns]\n",
    "y = df_final['target']\n",
    "feature_columns_after_permutation_test=var_selection_with_permutation(model,X,y,0.02)\n",
    "print(\n",
    "{\n",
    "    \"nb variables before permutation selection \" : len(feature_columns),\n",
    "    \"nb variables after permutation selection \" : len(feature_columns_after_permutation_test)\n",
    "\n",
    "}\n",
    ")\n",
    "model=MlFlowModel(\"rf-per-vars0.02-sqrt-logloss\",RandomForestClassifier(),max_features=\"sqrt\",criterion=\"log_loss\")\n",
    "model.train(df_final,feature_columns_after_permutation_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7367997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "experiment with this ntb more (first delete other experiments old)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
